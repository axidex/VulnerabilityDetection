import nltk
nltk.download('punkt')
from gensim.models import Word2Vec, KeyedVectors
import os.path
import pickle
import sys
from tqdm import tqdm

import concurrent.futures

all_words = []
    
mode = "withString" #default
if (len(sys.argv) > 1):
    mode = sys.argv[1]


def proccess_chunk(chunk: list) -> list:
    re = []
    for i in tqdm(chunk):
      re.append(nltk.word_tokenize(i))

    return re

# Loading the training corpus
print("Loading " + mode)  
with open('w2v/pythontraining' + '_'+mode+"_X", 'r') as file:
    pythondata = file.read().lower().replace('\n', ' ')

print("Length of the training file: " + str(len(pythondata)) + ".")
print("It contains " + str(pythondata.count(" ")) + " individual code tokens.")

# Preparing the dataset (or loading already processed dataset to not do everything again)
if (os.path.isfile('data/pythontraining_processed_' + mode)):
  with open ('data/pythontraining_processed_' + mode, 'rb') as fp:
    print(f"loaded data/pythontraining_processed_{mode}")
    all_words = pickle.load(fp)
  print("loaded processed model.")
else:  
  print("now processing...")
  processed = pythondata
  print('1')
  all_sentences = nltk.sent_tokenize(processed)
  print(len(all_sentences))
  chunk_size = len(all_sentences) // 10
  chunks = [all_sentences[i:i+chunk_size] for i in range(0, len(all_sentences), chunk_size)]
  all_words = []
  with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:  # Создание пула из 10 потоков
    all_words.extend(executor.map(proccess_chunk, chunks))
  for sent in tqdm(all_sentences):
    all_words.append(nltk.word_tokenize(sent))
  print(len(all_words))
  print("saving")
  with open('data/pythontraining_processed_' + mode, 'wb') as fp:
    pickle.dump(all_words, fp)

print("processed.\n")

#trying out different parameters
for mincount in [10,30,50,100,300,500,5000]:
  for iterationen in [1,5,10,30,50,100]:
    for s in [5,10,15,30,50,75,100,200,300]:

      print("\n\n" + mode + " W2V model with min count " + str(mincount) + " and " + str(iterationen) + " Iterationen and size " + str(s))
      fname = "w2v/word2vec_"+mode+str(mincount) + "-" + str(iterationen) +"-" + str(s)+ ".model"

      if (os.path.isfile(fname)):
        print("model already exists.")
        continue
      
      else:
        print("calculating model...")
        # training the model
        model = Word2Vec(all_words, size=s, min_count=mincount, iter=iterationen, workers = 4)  
        vocabulary = model.wv.vocab

        #print some examples
        
        #words = ["import", "true", "while", "if", "try", "in", "+", "x", "=", ":", "[", "print", "str", "count", "len", "where", "join", "split", "==", "raw_input"]
        #for similar in words:
        #  try:
        #    print("\n")
        #    print(similar)
        #    sim_words = model.wv.most_similar(similar)  
        #    print(sim_words)
        #    print("\n")
        #  except Exception as e:
        #    print(e)
        #    print("\n")

        #saving the model
        model.save(fname)



